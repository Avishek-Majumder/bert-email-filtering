{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2705c7a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# BERT Spam Detection Demo\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates how to load the fine-tuned BERT model from our repository for:\\n\",\n",
    "    \"\\n\",\n",
    "    \"> **“Harnessing BERT for Advanced Email Filtering in Cybersecurity”**  \\n\",\n",
    "    \"> IEEE Xplore: https://ieeexplore.ieee.org/abstract/document/11058531\\n\",\n",
    "    \"\\n\",\n",
    "    \"and run predictions on custom messages (SMS/email-like text).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Setup and Imports\\n\",\n",
    "    \"\\n\",\n",
    "    \"Make sure you have run BERT fine-tuning first (e.g., `python -m scripts.run_bert`),\\n\",\n",
    "    \"which will save the model under `experiments/bert/`.\\n\",\n",
    "    \"\\n\",\n",
    "    \"If you haven't trained yet, you can still run this notebook by loading a base model\\n\",\n",
    "    \"such as `bert-base-uncased`, but the predictions will not match our reported results.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"tags\": []\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"from typing import List\\n\",\n",
    "    \"\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\n\",\n",
    "    \"\\n\",\n",
    "    \"DEVICE = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\",\n",
    "    \"DEVICE\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Load the Fine-tuned BERT Model\\n\",\n",
    "    \"\\n\",\n",
    "    \"By default, we first try to load the fine-tuned model from `experiments/bert/`.\\n\",\n",
    "    \"If that directory is not found (e.g., you haven't trained yet), we fall back\\n\",\n",
    "    \"to the base `bert-base-uncased` model.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"tags\": []\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"MODEL_DIR = \\\"experiments/bert\\\"  # where train_bert.py saves the HF model\\n\",\n",
    "    \"BASE_MODEL_NAME = \\\"bert-base-uncased\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"if os.path.isdir(MODEL_DIR) and any(f.endswith(\\\".bin\\\") or f.endswith(\\\".safetensors\\\") for f in os.listdir(MODEL_DIR)):\\n\",\n",
    "    \"    print(f\\\"Loading fine-tuned model from: {MODEL_DIR}\\\")\\n\",\n",
    "    \"    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\\n\",\n",
    "    \"    model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"Fine-tuned model not found at '{MODEL_DIR}'. Loading base model: {BASE_MODEL_NAME}\\\")\\n\",\n",
    "    \"    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\\n\",\n",
    "    \"    model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL_NAME, num_labels=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"model.to(DEVICE)\\n\",\n",
    "    \"model.eval()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Helper Function for Inference\\n\",\n",
    "    \"\\n\",\n",
    "    \"We define a small helper that:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Tokenizes input texts.\\n\",\n",
    "    \"2. Runs them through BERT.\\n\",\n",
    "    \"3. Returns labels (`\\\"ham\\\"` / `\\\"spam\\\"`) and confidence scores.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We assume label index mapping:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- 0 → `ham`\\n\",\n",
    "    \"- 1 → `spam`\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"tags\": []\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import torch.nn.functional as F\\n\",\n",
    "    \"\\n\",\n",
    "    \"ID2LABEL = {0: \\\"ham\\\", 1: \\\"spam\\\"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"def predict_messages(texts: List[str]):\\n\",\n",
    "    \"    \\\"\\\"\\\"Run spam/ham prediction on a list of messages.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Returns a list of dicts: {\\\"text\\\", \\\"pred_label\\\", \\\"pred_index\\\", \\\"score\\\"}.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    if isinstance(texts, str):\\n\",\n",
    "    \"        texts = [texts]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    encodings = tokenizer(\\n\",\n",
    "    \"        texts,\\n\",\n",
    "    \"        padding=True,\\n\",\n",
    "    \"        truncation=True,\\n\",\n",
    "    \"        max_length=128,\\n\",\n",
    "    \"        return_tensors=\\\"pt\\\",\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    encodings = {k: v.to(DEVICE) for k, v in encodings.items()}\\n\",\n",
    "    \"\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        outputs = model(**encodings)\\n\",\n",
    "    \"        logits = outputs.logits\\n\",\n",
    "    \"        probs = F.softmax(logits, dim=-1)\\n\",\n",
    "    \"        scores, preds = torch.max(probs, dim=-1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    results = []\\n\",\n",
    "    \"    for text, idx, score in zip(texts, preds.cpu().tolist(), scores.cpu().tolist()):\\n\",\n",
    "    \"        label = ID2LABEL.get(idx, str(idx))\\n\",\n",
    "    \"        results.append(\\n\",\n",
    "    \"            {\\n\",\n",
    "    \"                \\\"text\\\": text,\\n\",\n",
    "    \"                \\\"pred_label\\\": label,\\n\",\n",
    "    \"                \\\"pred_index\\\": idx,\\n\",\n",
    "    \"                \\\"score\\\": float(score),\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"    return results\\n\",\n",
    "    \"\\n\",\n",
    "    \"def pretty_print_predictions(results):\\n\",\n",
    "    \"    for r in results:\\n\",\n",
    "    \"        print(\\\"------------------------------\\\")\\n\",\n",
    "    \"        print(f\\\"Text: {r['text']}\\\")\\n\",\n",
    "    \"        print(f\\\"Prediction: {r['pred_label']} (score={r['score']:.4f})\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Helper functions defined.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Try Some Example Messages\\n\",\n",
    "    \"\\n\",\n",
    "    \"Below we test the model with a small batch of messages, mixing benign and spammy content.\\n\",\n",
    "    \"\\n\",\n",
    "    \"If you have fine-tuned the model as in our experiments, the predictions should be aligned\\n\",\n",
    "    \"with our reported performance. With a base, non-fine-tuned model, the predictions will\\n\",\n",
    "    \"be mostly random.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"tags\": []\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"sample_texts = [\\n\",\n",
    "    \"    \\\"Hey, are we still meeting for lunch tomorrow?\\\",\\n\",\n",
    "    \"    \\\"Congratulations! You have won a $500 gift card. Click here to claim now!\\\",\\n\",\n",
    "    \"    \\\"Reminder: Your verification code is 392018. Do not share this code with anyone.\\\",\\n\",\n",
    "    \"    \\\"URGENT!! Your bank account has been suspended. Visit http://fake-bank-login.com to reactivate.\\\",\\n\",\n",
    "    \"    \\\"Can you send me the project report by tonight?\\\",\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"results = predict_messages(sample_texts)\\n\",\n",
    "    \"pretty_print_predictions(results)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Using the Model in Your Own Code\\n\",\n",
    "    \"\\n\",\n",
    "    \"To reuse the fine-tuned model outside this notebook, you can follow the same pattern\\n\",\n",
    "    \"in any Python script:\\n\",\n",
    "    \"\\n\",\n",
    "    \"```python\\n\",\n",
    "    \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn.functional as F\\n\",\n",
    "    \"\\n\",\n",
    "    \"tokenizer = AutoTokenizer.from_pretrained(\\\"experiments/bert\\\")\\n\",\n",
    "    \"model = AutoModelForSequenceClassification.from_pretrained(\\\"experiments/bert\\\")\\n\",\n",
    "    \"model.eval()\\n\",\n",
    "    \"\\n\",\n",
    "    \"def predict_one(text: str):\\n\",\n",
    "    \"    enc = tokenizer(text, return_tensors=\\\"pt\\\", truncation=True, padding=True)\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        logits = model(**enc).logits\\n\",\n",
    "    \"        probs = F.softmax(logits, dim=-1)\\n\",\n",
    "    \"        score, pred = torch.max(probs, dim=-1)\\n\",\n",
    "    \"    return int(pred.item()), float(score.item())\\n\",\n",
    "    \"```\\n\",\n",
    "    \"\\n\",\n",
    "    \"You can embed this in a web service, API, or batch-scoring pipeline as needed.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.9\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
